<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# ì‹œë§¨í‹± ì„¸ê·¸ë©˜í…Œì´ì…˜ [[semantic-segmentation]]

[[open-in-colab]]

<Youtube id="dKE8SIt9C-w"/>

ì‹œë§¨í‹± ì„¸ê·¸ë©˜í…Œì´ì…˜ì€ ì´ë¯¸ì§€ì˜ ê° í”½ì…€ì— ë ˆì´ë¸”ì´ë‚˜ í´ë˜ìŠ¤ë¥¼ í• ë‹¹í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ì´ ìˆì§€ë§Œ, ì‹œë§¨í‹± ì„¸ê·¸ë©˜í…Œì´ì…˜ì˜ ê²½ìš° ê°™ì€ ê°ì²´ì—ëŠ” ì„œë¡œ ë‹¤ë¥¸ ê°œì²´ë“¤ì„ êµ¬ë³„í•˜ì§€ ì•Šê³  ë ˆì´ë¸”ì„ í• ë‹¹í•©ë‹ˆë‹¤. ì¦‰, "ìë™ì°¨-1"ê³¼ "ìë™ì°¨-2" ëŒ€ì‹ ì— ë‘ ê°œì²´ì— ëª¨ë‘ "ìë™ì°¨" ë¼ë²¨ì´ í• ë‹¹ë©ë‹ˆë‹¤. ì‹œë§¨í‹± ì„¸ê·¸ë©˜í…Œì´ì…˜ì€ ë³´í–‰ìì™€ êµí†µ ì •ë³´ë¥¼ ì‹ë³„í•˜ëŠ” ììœ¨ì£¼í–‰ ìë™ì°¨ë¥¼ í›ˆë ¨ì‹œí‚¤ê±°ë‚˜, ì˜ë£Œ ì˜ìƒì—ì„œ ì„¸í¬ì˜ ì´ìƒì„ ì‹ë³„í•˜ê±°ë‚˜, ê·¸ë¦¬ê³  ìœ„ì„± ì˜ìƒì„ í†µí•œ í™˜ê²½ ë³€í™” ëª¨ë‹ˆí„°ë§í•˜ëŠ” ë“± ì‹¤ì œ ì‘ìš© ë¶„ì•¼ì—ì„œ ë‹¤ì–‘í•˜ê²Œ í™œìš©ë©ë‹ˆë‹¤.

ì´ ê°€ì´ë“œì—ì„œëŠ” ë‹¤ìŒì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ì•ˆë‚´í•©ë‹ˆë‹¤:

1. [SegFormer](https://huggingface.co/docs/transformers/main/en/model_doc/segformer#segformer)ë¥¼ [SceneParse150](https://huggingface.co/datasets/scene_parse_150) ë°ì´í„°ì…‹ì—ì„œ íŒŒì¸íŠœë‹í•˜ëŠ” ë°©ë²•.
2. íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ì¶”ë¡ ì— ì‚¬ìš©í•˜ëŠ” ë°©ë²•.

<Tip>
ì´ íŠœí† ë¦¬ì–¼ì—ì„œ ì„¤ëª…í•˜ëŠ” ì‘ì—…ì€ ë‹¤ìŒ ëª¨ë¸ ì•„í‚¤í…ì²˜ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤:

<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->

[BEiT](../../en/model_doc/beit), [Data2VecVision](../../en/model_doc/data2vec-vision), [DPT](../../en/model_doc/dpt), [MobileNetV2](../../en/model_doc/mobilenet_v2), [MobileViT](../../en/model_doc/mobilevit), [MobileViTV2](../../en/model_doc/mobilevitv2), [SegFormer](../../en/model_doc/segformer), [UPerNet](../../en/model_doc/upernet)

<!--End of the generated tip-->

</Tip>

ì‹œì‘í•˜ê¸° ì „ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ëª¨ë‘ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:

```bash
pip install -q datasets transformers evaluate
```

ëª¨ë¸ì„ ì»¤ë®¤ë‹ˆí‹°ì™€ ê³µìœ í•˜ë ¤ë©´ Hugging Face ê³„ì •ì— ë¡œê·¸ì¸í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤. ë¡œê·¸ì¸ì„ ìš”ì²­ë°›ìœ¼ë©´ í† í°ì„ ì…ë ¥í•˜ì—¬ ë¡œê·¸ì¸í•˜ì„¸ìš”:

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## SceneParse150 ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸° [[load-sceneparse150-dataset]]

ë¨¼ì €, ğŸ¤— Datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì‘ì€ SceneParse150 ë°ì´í„°ì…‹ í•˜ìœ„ ì§‘í•©ì„ ë¡œë“œí•©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ì „ì²´ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ê¸° ì „ì— ì‹¤í—˜í•´ë³´ê³  ëª¨ë“  ê²ƒì´ ì˜¬ë°”ë¥´ê²Œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```py
>>> from datasets import load_dataset

>>> ds = load_dataset("scene_parse_150", split="train[:50]")
```

ë°ì´í„°ì…‹ì˜ `train` ë¶„í• ì„ [`~datasets.Dataset.train_test_split`] ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„í• í•©ë‹ˆë‹¤.

```py
>>> ds = ds.train_test_split(test_size=0.2)
>>> train_ds = ds["train"]
>>> test_ds = ds["test"]
```

ê·¸ëŸ° ë‹¤ìŒ, ì˜ˆì‹œë¥¼ ì‚´í´ë³´ì„¸ìš”:

```py
>>> train_ds[0]
{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x683 at 0x7F9B0C201F90>,
 'annotation': <PIL.PngImagePlugin.PngImageFile image mode=L size=512x683 at 0x7F9B0C201DD0>,
 'scene_category': 368}
```

- `image`: ì¥ë©´ì˜ PIL ì´ë¯¸ì§€.
- `annotation`: ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§µì˜ PIL ì´ë¯¸ì§€, ëª¨ë¸ì˜ ëª©í‘œë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
- `scene_category`: "ë¶€ì—Œ"ì´ë‚˜ "ì‚¬ë¬´ì‹¤"ê³¼ ê°™ì€ ì´ë¯¸ì§€ ì¥ë©´ì„ ë‚˜íƒ€ë‚´ëŠ” ì¹´í…Œê³ ë¦¬ IDì…ë‹ˆë‹¤. ì´ ê°€ì´ë“œì—ì„œëŠ” `image`ì™€ `annotation`ë§Œ í•„ìš”í•©ë‹ˆë‹¤. ë‘ ì´ë¯¸ì§€ ëª¨ë‘ PIL ì´ë¯¸ì§€ì…ë‹ˆë‹¤.

ì´í›„ ëª¨ë¸ ì„¤ì • ì‹œ ë ˆì´ë¸” IDë¥¼ ë ˆì´ë¸” í´ë˜ìŠ¤ë¡œ ë§¤í•‘í•˜ëŠ” ì‚¬ì „ì„ ë§Œë“¤ í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤. ë§¤í•‘ì€ í—ˆë¸Œì—ì„œ ë‹¤ìš´ë¡œë“œí•˜ì—¬ `id2label` ë° `label2id` ì‚¬ì „ì„ ìƒì„±í•©ë‹ˆë‹¤.

```py
>>> import json
>>> from huggingface_hub import cached_download, hf_hub_url

>>> repo_id = "huggingface/label-files"
>>> filename = "ade20k-id2label.json"
>>> id2label = json.load(open(cached_download(hf_hub_url(repo_id, filename, repo_type="dataset")), "r"))
>>> id2label = {int(k): v for k, v in id2label.items()}
>>> label2id = {v: k for k, v in id2label.items()}
>>> num_labels = len(id2label)
```

## ë°ì´í„° ì „ì²˜ë¦¬ [[preprocess]]

ë‹¤ìŒ ë‹¨ê³„ëŠ” SegFormer ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì™€ ì–´ë…¸í…Œì´ì…˜ì„ ëª¨ë¸ì— ì…ë ¥ ê°€ëŠ¥í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì¼ë¶€ ë°ì´í„°ì…‹ì€ ë°°ê²½ í´ë˜ìŠ¤ë¡œ ì œë¡œ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ê°€ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ 150ê°œì˜ í´ë˜ìŠ¤ ì¤‘ ë°°ê²½ í´ë˜ìŠ¤ê°€ ì—†ìœ¼ë¯€ë¡œ ëª¨ë“  ë ˆì´ë¸”ì—ì„œ 1ì„ ë¹¼ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ `reduce_labels=True`ë¡œ ì„¤ì •í•˜ì—¬ ì œë¡œ ì¸ë±ìŠ¤ë¥¼ `255`ë¡œ ëŒ€ì²´í•˜ì—¬ SegFormer ì†ì‹¤ í•¨ìˆ˜ì—ì„œ ë°°ê²½ í´ë˜ìŠ¤ë¥¼ ë¬´ì‹œí•˜ë„ë¡ í•©ë‹ˆë‹¤.

```py
>>> from transformers import AutoImageProcessor

>>> checkpoint = "nvidia/mit-b0"
>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint, reduce_labels=True)
```

<frameworkcontent>
<pt>

ëª¨ë¸ì´ ê³¼ì í•©ì„ í”¼í•˜ê¸° ìœ„í•´ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ì— ë°ì´í„° ì¦ê°•ì„ ì ìš©í•˜ëŠ” ê²ƒì€ í”í•œ ì¼ì…ë‹ˆë‹¤. ì´ ê°€ì´ë“œì—ì„œëŠ” [torchvision](https://pytorch.org/vision/stable/index.html) ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ [`ColorJitter`](https://pytorch.org/vision/stable/generated/torchvision.transforms.ColorJitter.html) í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì˜ ìƒ‰ìƒ ì†ì„±ì„ ë¬´ì‘ìœ„ë¡œ ë³€ê²½í•  ê²ƒì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ë‹¤ë¥¸ ì´ë¯¸ì§€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•´ë„ ë¬´ë°©í•©ë‹ˆë‹¤.

```py
>>> from torchvision.transforms import ColorJitter

>>> jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)
```

ë¨¼ì €, ì´ë¯¸ì§€ë¥¼ `pixel_values`ë¡œ ë³€í™˜í•˜ê³  ì–´ë…¸í…Œì´ì…˜ì„ `labels`ë¡œ ë³€í™˜í•˜ëŠ” ë‘ ê°œì˜ ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. í›ˆë ¨ ì„¸íŠ¸ì— ëŒ€í•´ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œì— `jitter`ë¥¼ ì ìš©í•œ ë‹¤ìŒ ì´ë¯¸ì§€ë¥¼ ì œê³µí•©ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— ëŒ€í•´ì„œëŠ” ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œê°€ ì´ë¯¸ì§€ë¥¼ ì˜ë¼ë‚´ê³  ì •ê·œí™”í•˜ë©° í…ŒìŠ¤íŠ¸ ì¤‘ì—ëŠ” ë°ì´í„° ì¦ê°•ì„ ì ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

```py
>>> def train_transforms(example_batch):
...     images = [jitter(x) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs


>>> def val_transforms(example_batch):
...     images = [x for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs
```

ë°ì´í„°ì…‹ ì „ì²´ì— ì „ì²˜ë¦¬ ë³€í™˜ì„ ì ìš©í•˜ë ¤ë©´ ğŸ¤— Datasets [`~datasets.Dataset.set_transform`] í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
ì´ ë³€í™˜ì€ ì‹¤ì‹œê°„ìœ¼ë¡œ ì ìš©ë˜ë©° ë””ìŠ¤í¬ ê³µê°„ì„ ë” ì ê²Œ ì‚¬ìš©í•©ë‹ˆë‹¤:

```py
>>> train_ds.set_transform(train_transforms)
>>> test_ds.set_transform(val_transforms)
```

</pt>
</frameworkcontent>

<frameworkcontent>
<tf>
ë°ì´í„° ì¦ê°•ì„ ì ìš©í•˜ì—¬ ëª¨ë¸ì´ ê³¼ì í•©ì„ ë” ì˜ í”¼í•˜ë„ë¡ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ì„ ë³€í™˜í•˜ëŠ” ê²ƒì€ ì¼ë°˜ì ì…ë‹ˆë‹¤.
ì´ ì•ˆë‚´ì„œì—ì„œëŠ” [`tf.image`](https://www.tensorflow.org/api_docs/python/tf/image)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì˜ ìƒ‰ìƒ íŠ¹ì„±ì„ ë¬´ì‘ìœ„ë¡œ ë³€ê²½í•  ê²ƒì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ë‹¤ë¥¸ ì´ë¯¸ì§€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•´ë„ ê´œì°®ìŠµë‹ˆë‹¤.
ë‹¤ìŒì€ ë‘ ê°€ì§€ ë‹¤ë¥¸ ë³€í™˜ í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ì„¸ìš”:
- ì´ë¯¸ì§€ ì¦ê°•ì„ í¬í•¨í•˜ëŠ” í›ˆë ¨ ë°ì´í„° ë³€í™˜
- ì»´í“¨í„° ë¹„ì „ ëª¨ë¸ì—ì„œëŠ” ì±„ë„ ìˆœì„œë¥¼ ë¨¼ì € ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë¯€ë¡œ ê²€ì¦ ë°ì´í„° ë³€í™˜ì€ ì´ë¯¸ì§€ë¥¼ ì „ì¹˜(transpose)í•˜ëŠ” ê²ƒë§Œ í¬í•¨í•©ë‹ˆë‹¤.

```py
>>> import tensorflow as tf


>>> def aug_transforms(image):
...     image = tf.keras.utils.img_to_array(image)
...     image = tf.image.random_brightness(image, 0.25)
...     image = tf.image.random_contrast(image, 0.5, 2.0)
...     image = tf.image.random_saturation(image, 0.75, 1.25)
...     image = tf.image.random_hue(image, 0.1)
...     image = tf.transpose(image, (2, 0, 1))
...     return image


>>> def transforms(image):
...     image = tf.keras.utils.img_to_array(image)
...     image = tf.transpose(image, (2, 0, 1))
...     return image
```

ì´ì œ ì´ë¯¸ì§€ì™€ ì–´ë…¸í…Œì´ì…˜ì„ ëª¨ë¸ì— ì…ë ¥í•  ìˆ˜ ìˆë„ë¡ ë‘ ê°€ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ì´ë¯¸ì§€ë¥¼ `pixel_values`ë¡œ ë³€í™˜í•˜ê³  ì–´ë…¸í…Œì´ì…˜ì„ `labels`ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. í›ˆë ¨ ì„¸íŠ¸ì˜ ê²½ìš° ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œê°€ ì´ë¯¸ì§€ì— `jitter`ë¥¼ ì ìš©í•œ í›„ ì´ë¯¸ì§€ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ê²½ìš° ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œê°€ ì´ë¯¸ì§€ë¥¼ ì˜ë¼ë‚´ê³  ì •ê·œí™”í•˜ë©° í…ŒìŠ¤íŠ¸ ì¤‘ì—ëŠ” ë°ì´í„° ì¦ê°•ì„ ì ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

```py
>>> def train_transforms(example_batch):
...     images = [aug_transforms(x.convert("RGB")) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs


>>> def val_transforms(example_batch):
...     images = [transforms(x.convert("RGB")) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs
```

ì „ì²´ ë°ì´í„°ì…‹ì— ì „ì²˜ë¦¬ ë³€í™˜ì„ ì ìš©í•˜ë ¤ë©´ ğŸ¤— Datasets [`~datasets.Dataset.set_transform`] í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
ë³€í™˜ì€ ì‹¤ì‹œê°„ìœ¼ë¡œ ì ìš©ë˜ë©° ë””ìŠ¤í¬ ê³µê°„ì„ ì ê²Œ ì‚¬ìš©í•©ë‹ˆë‹¤:

```py
>>> train_ds.set_transform(train_transforms)
>>> test_ds.set_transform(val_transforms)
```
</tf>
</frameworkcontent>

## í‰ê°€ [[evaluate]]

í›ˆë ¨ ì¤‘ì— ë©”íŠ¸ë¦­ì„ í¬í•¨í•˜ëŠ” ê²ƒì€ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. ë¹ ë¥´ê²Œ í‰ê°€ ë©”ì„œë“œë¥¼ ë¡œë“œí•˜ê¸° ìœ„í•´ ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ì‘ì—…ì„ ìœ„í•´ [í‰ê·  IoU(Intersection over Union)](https://huggingface.co/spaces/evaluate-metric/accuracy) ë©”íŠ¸ë¦­ì„ ë¡œë“œí•˜ì„¸ìš” (ë” ë§ì€ ì •ë³´ë¥¼ ì•Œì•„ë³´ë ¤ë©´ ğŸ¤— Evaluate [ë¹ ë¥¸ íˆ¬ì–´](https://huggingface.co/docs/evaluate/a_quick_tour)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”):

```py
>>> import evaluate

>>> metric = evaluate.load("mean_iou")
```

ê·¸ëŸ° ë‹¤ìŒ [`~evaluate.EvaluationModule.compute`] ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ë¥¼ ìƒì„±í•˜ì„¸ìš”. ì˜ˆì¸¡ê°’ì„ ë¨¼ì € ë¡œì§“ìœ¼ë¡œ ë³€í™˜í•˜ê³ , [`~evaluate.EvaluationModule.compute`]ì„ í˜¸ì¶œí•˜ê¸° ì „ì— ë ˆì´ë¸” í¬ê¸°ì™€ ì¼ì¹˜í•˜ë„ë¡ ì¬êµ¬ì„±í•´ì•¼ í•©ë‹ˆë‹¤:

<frameworkcontent>
<pt>

```py
>>> import numpy as np
>>> import torch
>>> from torch import nn

>>> def compute_metrics(eval_pred):
...     with torch.no_grad():
...         logits, labels = eval_pred
...         logits_tensor = torch.from_numpy(logits)
...         logits_tensor = nn.functional.interpolate(
...             logits_tensor,
...             size=labels.shape[-2:],
...             mode="bilinear",
...             align_corners=False,
...         ).argmax(dim=1)

...         pred_labels = logits_tensor.detach().cpu().numpy()
...         metrics = metric.compute(
...             predictions=pred_labels,
...             references=labels,
...             num_labels=num_labels,
...             ignore_index=255,
...             reduce_labels=False,
...         )
...         for key, value in metrics.items():
...             if type(value) is np.ndarray:
...                 metrics[key] = value.tolist()
...         return metrics
```

</pt>
</frameworkcontent>


<frameworkcontent>
<tf>

```py
>>> def compute_metrics(eval_pred):
...     logits, labels = eval_pred
...     logits = tf.transpose(logits, perm=[0, 2, 3, 1])
...     logits_resized = tf.image.resize(
...         logits,
...         size=tf.shape(labels)[1:],
...         method="bilinear",
...     )

...     pred_labels = tf.argmax(logits_resized, axis=-1)
...     metrics = metric.compute(
...         predictions=pred_labels,
...         references=labels,
...         num_labels=num_labels,
...         ignore_index=-1,
...         reduce_labels=image_processor.do_reduce_labels,
...     )

...     per_category_accuracy = metrics.pop("per_category_accuracy").tolist()
...     per_category_iou = metrics.pop("per_category_iou").tolist()

...     metrics.update({f"accuracy_{id2label[i]}": v for i, v in enumerate(per_category_accuracy)})
...     metrics.update({f"iou_{id2label[i]}": v for i, v in enumerate(per_category_iou)})
...     return {"val_" + k: v for k, v in metrics.items()}
```

</tf>
</frameworkcontent>

`compute_metrics` í•¨ìˆ˜ê°€ ì¤€ë¹„ë˜ì—ˆìœ¼ë¯€ë¡œ ëª¨ë¸ ì„¤ì • ì‹œ ì´ í•¨ìˆ˜ë¥¼ ë‹¤ì‹œ ì‚¬ìš©í•˜ê²Œ ë  ê²ƒì…ë‹ˆë‹¤.

## í›ˆë ¨ [[train]]
<frameworkcontent>
<pt>
<Tip>

[`Trainer`]ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ëŠ” ë°©ë²•ì— ìµìˆ™í•˜ì§€ ì•Šë‹¤ë©´, [ì—¬ê¸°](../training#finetune-with-trainer)ì˜ ê¸°ë³¸ íŠœí† ë¦¬ì–¼ì„ í™•ì¸í•˜ì„¸ìš”!

</Tip>

ì´ì œ ëª¨ë¸ì„ í›ˆë ¨í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤! [`AutoModelForSemanticSegmentation`]ë¥¼ ì‚¬ìš©í•˜ì—¬ SegFormer ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ë ˆì´ë¸” IDì™€ ë ˆì´ë¸” í´ë˜ìŠ¤ ê°„ì˜ ë§¤í•‘ì„ ëª¨ë¸ì— ì „ë‹¬í•˜ì„¸ìš”:

```py
>>> from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer

>>> model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id=label2id)
```

ì´ ì‹œì ì—ì„œ ì„¸ ê°€ì§€ ë‹¨ê³„ë§Œ ë‚¨ì•˜ìŠµë‹ˆë‹¤:

1. [`TrainingArguments`]ì—ì„œ í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì •ì˜í•˜ì„¸ìš”. ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì—´ì„ ì œê±°í•˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ì„¸ìš”. ì´ë ‡ê²Œ í•˜ë©´ `image` ì—´ì´ ì‚­ì œë©ë‹ˆë‹¤. `image` ì—´ ì—†ì´ `pixel_values`ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. `remove_unused_columns=False`ë¡œ ì„¤ì •í•˜ì—¬ ì´ ë™ì‘ì„ ë°©ì§€í•˜ì„¸ìš”! ë˜í•œ ìœ ì¼í•œ í•„ìˆ˜ ë§¤ê°œë³€ìˆ˜ëŠ” ëª¨ë¸ì„ ì €ì¥í•  ìœ„ì¹˜ë¥¼ ì§€ì •í•˜ëŠ” `output_dir`ì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì„ í—ˆë¸Œë¡œ ì—…ë¡œë“œí•˜ë ¤ë©´ `push_to_hub=True`ë¡œ ì„¤ì •í•˜ì„¸ìš” (ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ë ¤ë©´ Hugging Faceì— ë¡œê·¸ì¸í•´ì•¼ í•©ë‹ˆë‹¤). ê° ì—í¬í¬ì˜ ëì—ì„œ [`Trainer`]ëŠ” IoU ë©”íŠ¸ë¦­ì„ í‰ê°€í•˜ê³  í›ˆë ¨ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
2. í›ˆë ¨ ì¸ìë¥¼ [`Trainer`]ì— ëª¨ë¸, ë°ì´í„°ì…‹, í† í¬ë‚˜ì´ì €, ë°ì´í„° ì½œë ˆì´í„° ë° `compute_metrics` í•¨ìˆ˜ì™€ í•¨ê»˜ ì „ë‹¬í•˜ì„¸ìš”.
3. [`~Trainer.train`]ì„ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ì„¸ìš”.

```py
>>> training_args = TrainingArguments(
...     output_dir="segformer-b0-scene-parse-150",
...     learning_rate=6e-5,
...     num_train_epochs=50,
...     per_device_train_batch_size=2,
...     per_device_eval_batch_size=2,
...     save_total_limit=3,
...     evaluation_strategy="steps",
...     save_strategy="steps",
...     save_steps=20,
...     eval_steps=20,
...     logging_steps=1,
...     eval_accumulation_steps=5,
...     remove_unused_columns=False,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=train_ds,
...     eval_dataset=test_ds,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

í›ˆë ¨ì´ ì™„ë£Œë˜ë©´ [`~transformers.Trainer.push_to_hub`] ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í—ˆë¸Œì— ê³µìœ í•˜ì„¸ìš”. ì´ë ‡ê²Œ í•˜ë©´ ëˆ„êµ¬ë‚˜ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
>>> trainer.push_to_hub()
```
</pt>
</frameworkcontent>

<frameworkcontent>
<tf>
<Tip>

ì¼€ë¼ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ëŠ” ë°©ë²•ì— ìµìˆ™í•˜ì§€ ì•Šë‹¤ë©´, [ê¸°ë³¸ íŠœí† ë¦¬ì–¼](./training#train-a-tensorflow-model-with-keras)ì„ ë¨¼ì € í™•ì¸í•˜ì„¸ìš”!

</Tip>

TensorFlowì—ì„œ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ë ¤ë©´ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¥´ì„¸ìš”:
1. í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì •ì˜í•˜ê³  ì˜µí‹°ë§ˆì´ì €ì™€ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ì„ ì„¤ì •í•©ë‹ˆë‹¤.
2. ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•©ë‹ˆë‹¤.
3. ğŸ¤— Datasetì„ `tf.data.Dataset` í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
4. ëª¨ë¸ì„ ì»´íŒŒì¼í•©ë‹ˆë‹¤.
5. ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•˜ê³  ëª¨ë¸ì„ ğŸ¤— Hubì— ì—…ë¡œë“œí•˜ê¸° ìœ„í•œ ì½œë°±ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
6. `fit()` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

ë¨¼ì €, í•˜ì´í¼íŒŒë¼ë¯¸í„°, ì˜µí‹°ë§ˆì´ì € ë° í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ì„ ì •ì˜í•˜ì„¸ìš”:

```py
>>> from transformers import create_optimizer

>>> batch_size = 2
>>> num_epochs = 50
>>> num_train_steps = len(train_ds) * num_epochs
>>> learning_rate = 6e-5
>>> weight_decay_rate = 0.01

>>> optimizer, lr_schedule = create_optimizer(
...     init_lr=learning_rate,
...     num_train_steps=num_train_steps,
...     weight_decay_rate=weight_decay_rate,
...     num_warmup_steps=0,
... )
```

ê·¸ëŸ° ë‹¤ìŒ, ë ˆì´ë¸” ë§¤í•‘ê³¼ í•¨ê»˜ [`TFAutoModelForSemanticSegmentation`]ë¥¼ ì‚¬ìš©í•˜ì—¬ SegFormerë¥¼ ë¡œë“œí•˜ê³  ì˜µí‹°ë§ˆì´ì €ì™€ í•¨ê»˜ ì»´íŒŒì¼í•˜ì„¸ìš”.
Transformer ëª¨ë¸ì€ ëª¨ë‘ ê¸°ë³¸ ì‘ì—… ê´€ë ¨ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë¯€ë¡œ í•„ìš”í•œ ê²½ìš° ì§€ì •í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤:

```py
>>> from transformers import TFAutoModelForSemanticSegmentation

>>> model = TFAutoModelForSemanticSegmentation.from_pretrained(
...     checkpoint,
...     id2label=id2label,
...     label2id=label2id,
... )
>>> model.compile(optimizer=optimizer)  # No loss argument!
```

ë°ì´í„°ì…‹ì„ [`~datasets.Dataset.to_tf_dataset`] ë° [`DefaultDataCollator`]ë¥¼ ì‚¬ìš©í•˜ì—¬ `tf.data.Dataset` í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”:

```py
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator(return_tensors="tf")

>>> tf_train_dataset = train_ds.to_tf_dataset(
...     columns=["pixel_values", "label"],
...     shuffle=True,
...     batch_size=batch_size,
...     collate_fn=data_collator,
... )

>>> tf_eval_dataset = test_ds.to_tf_dataset(
...     columns=["pixel_values", "label"],
...     shuffle=True,
...     batch_size=batch_size,
...     collate_fn=data_collator,
... )
```

ì˜ˆì¸¡ì—ì„œ ì •í™•ë„ë¥¼ ê³„ì‚°í•˜ê³  ëª¨ë¸ì„ ğŸ¤— Hubì— ì—…ë¡œë“œí•˜ë ¤ë©´ [ì¼€ë¼ìŠ¤ ì½œë°±](../main_classes/keras_callbacks)ì„ ì‚¬ìš©í•˜ì„¸ìš”.
`compute_metrics` í•¨ìˆ˜ë¥¼ [`KerasMetricCallback`]ì— ì „ë‹¬í•˜ê³  [`PushToHubCallback`]ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ì„¸ìš”:

```py
>>> from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback

>>> metric_callback = KerasMetricCallback(
...     metric_fn=compute_metrics, eval_dataset=tf_eval_dataset, batch_size=batch_size, label_cols=["labels"]
... )

>>> push_to_hub_callback = PushToHubCallback(output_dir="scene_segmentation", tokenizer=image_processor)

>>> callbacks = [metric_callback, push_to_hub_callback]
```

ë§ˆì§€ë§‰ìœ¼ë¡œ, ëª¨ë¸ì„ í›ˆë ¨í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤! `fit()`ì„ í˜¸ì¶œí•˜ì—¬ í›ˆë ¨ ë° ê²€ì¦ ë°ì´í„°ì…‹, ì—í¬í¬ ìˆ˜ ë° ì½œë°±ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ì„¸ìš”:

```py
>>> model.fit(
...     tf_train_dataset,
...     validation_data=tf_eval_dataset,
...     callbacks=callbacks,
...     epochs=num_epochs,
... )
```

ì¶•í•˜í•©ë‹ˆë‹¤! ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ê³  ğŸ¤— Hubì— ê³µìœ í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ì œ ì¶”ë¡ ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!
</tf>
</frameworkcontent>


## ì¶”ë¡  [[inference]]

ì¢‹ìŠµë‹ˆë‹¤, ì´ì œ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í–ˆìœ¼ë¯€ë¡œ ì¶”ë¡ ì— í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

ì¶”ë¡ ìš© ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¤ì„¸ìš”:

```py
>>> image = ds[0]["image"]
>>> image
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-image.png" alt="ì¹¨ì‹¤ ì´ë¯¸ì§€"/>
</div>

<frameworkcontent>
<pt>
íŒŒì¸íŠœë‹í•œ ëª¨ë¸ì„ ì¶”ë¡ ì— ê°„ë‹¨íˆ í™œìš©í•˜ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ [`pipeline`]ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜ì„ ìœ„í•œ `pipeline`ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ê³  ì´ë¯¸ì§€ë¥¼ ì „ë‹¬í•˜ì„¸ìš”:

```py
>>> from transformers import pipeline

>>> segmenter = pipeline("image-segmentation", model="my_awesome_seg_model")
>>> segmenter(image)
[{'score': None,
  'label': 'wall',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062690>},
 {'score': None,
  'label': 'sky',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062A50>},
 {'score': None,
  'label': 'floor',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062B50>},
 {'score': None,
  'label': 'ceiling',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062A10>},
 {'score': None,
  'label': 'bed ',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062E90>},
 {'score': None,
  'label': 'windowpane',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062390>},
 {'score': None,
  'label': 'cabinet',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062550>},
 {'score': None,
  'label': 'chair',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062D90>},
 {'score': None,
  'label': 'armchair',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062E10>}]
```

ì›í•˜ëŠ” ê²½ìš° `pipeline`ì˜ ê²°ê³¼ë¥¼ ì§ì ‘ ì¬í˜„í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ê³  `pixel_values`ë¥¼ GPUì— ë„£ì€ ë‹¤ìŒ ëª¨ë¸ì— ì…ë ¥í•˜ì„¸ìš”:

```py
>>> device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # use GPU if available, otherwise use a CPU
>>> encoding = image_processor(image, return_tensors="pt")
>>> pixel_values = encoding.pixel_values.to(device)
```

ì…ë ¥ì„ ëª¨ë¸ì— ì „ë‹¬í•˜ê³  `logits`ì„ ì–»ìœ¼ì„¸ìš”:

```py
>>> outputs = model(pixel_values=pixel_values)
>>> logits = outputs.logits.cpu()
```

ê·¸ëŸ° ë‹¤ìŒ `logits`ì„ ì›ë˜ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ë‹¤ì‹œ ìŠ¤ì¼€ì¼ë§í•˜ì„¸ìš”:

```py
>>> upsampled_logits = nn.functional.interpolate(
...     logits,
...     size=image.size[::-1],
...     mode="bilinear",
...     align_corners=False,
... )

>>> pred_seg = upsampled_logits.argmax(dim=1)[0]
```

</pt>
</frameworkcontent>

<frameworkcontent>
<tf>
ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ í•´ ë³´ë ¤ë©´ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ê³  TensorFlow í…ì„œë¡œ ì…ë ¥ì„ ë°˜í™˜í•˜ì„¸ìš”:

```py
>>> from transformers import AutoImageProcessor

>>> image_processor = AutoImageProcessor.from_pretrained("MariaK/scene_segmentation")
>>> inputs = image_processor(image, return_tensors="tf")
```

ëª¨ë¸ì— ì…ë ¥ì„ ì „ë‹¬í•˜ê³  `logits`ì„ ì–»ìœ¼ì„¸ìš”:

```py
>>> from transformers import TFAutoModelForSemanticSegmentation

>>> model = TFAutoModelForSemanticSegmentation.from_pretrained("MariaK/scene_segmentation")
>>> logits = model(**inputs).logits
```

ê·¸ëŸ° ë‹¤ìŒ `logits`ì„ ì›ë˜ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ë‹¤ì‹œ ìŠ¤ì¼€ì¼ë§í•˜ê³  í´ë˜ìŠ¤ ì°¨ì›ì—ì„œ argmaxë¥¼ ì ìš©í•˜ì„¸ìš”:
```py
>>> logits = tf.transpose(logits, [0, 2, 3, 1])

>>> upsampled_logits = tf.image.resize(
...     logits,
...     # We reverse the shape of `image` because `image.size` returns width and height.
...     image.size[::-1],
... )

>>> pred_seg = tf.math.argmax(upsampled_logits, axis=-1)[0]
```

</tf>
</frameworkcontent>

ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ë ¤ë©´ [ë°ì´í„°ì…‹ ì»¬ëŸ¬ íŒ”ë ˆíŠ¸](https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/deeplab/utils/get_dataset_colormap.py#L51)ë¥¼ ë¶ˆëŸ¬ì™€ ê° í´ë˜ìŠ¤ë¥¼ RGB ê°’ì— ë§¤í•‘í•˜ëŠ” `ade_palette()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ê·¸ëŸ° ë‹¤ìŒ ì´ë¯¸ì§€ì™€ ì˜ˆì¸¡ëœ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§µì„ ê²°í•©í•˜ì—¬ í”Œë¡¯í•˜ì„¸ìš”:

```py
>>> import matplotlib.pyplot as plt
>>> import numpy as np

>>> color_seg = np.zeros((pred_seg.shape[0], pred_seg.shape[1], 3), dtype=np.uint8)
>>> palette = np.array(ade_palette())
>>> for label, color in enumerate(palette):
...     color_seg[pred_seg == label, :] = color
>>> color_seg = color_seg[..., ::-1]  # convert to BGR

>>> img = np.array(image) * 0.5 + color_seg * 0.5  # plot the image with the segmentation map
>>> img = img.astype(np.uint8)

>>> plt.figure(figsize=(15, 10))
>>> plt.imshow(img)
>>> plt.show()
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-preds.png" alt="ì¹¨ì‹¤ ì´ë¯¸ì§€ ìœ„ì— ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§µ ê²¹ì³ ë³´ê¸°"/>
</div>
