<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# 시맨틱 세그멘테이션 [[semantic-segmentation]]

[[open-in-colab]]

<Youtube id="dKE8SIt9C-w"/>

시맨틱 세그멘테이션은 이미지의 각 픽셀에 레이블이나 클래스를 할당하는 작업입니다. 여러 종류의 세그멘테이션이 있지만, 시맨틱 세그멘테이션의 경우 같은 객체에는 서로 다른 개체들을 구별하지 않고 레이블을 할당합니다. 즉, "자동차-1"과 "자동차-2" 대신에 두 개체에 모두 "자동차" 라벨이 할당됩니다. 시맨틱 세그멘테이션은 보행자와 교통 정보를 식별하는 자율주행 자동차를 훈련시키거나, 의료 영상에서 세포의 이상을 식별하거나, 그리고 위성 영상을 통한 환경 변화 모니터링하는 등 실제 응용 분야에서 다양하게 활용됩니다.

이 가이드에서는 다음을 수행하는 방법을 안내합니다:

1. [SegFormer](https://huggingface.co/docs/transformers/main/en/model_doc/segformer#segformer)를 [SceneParse150](https://huggingface.co/datasets/scene_parse_150) 데이터셋에서 파인튜닝하는 방법.
2. 파인튜닝된 모델을 추론에 사용하는 방법.

<Tip>
이 튜토리얼에서 설명하는 작업은 다음 모델 아키텍처에서 지원됩니다:

<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->

[BEiT](../../en/model_doc/beit), [Data2VecVision](../../en/model_doc/data2vec-vision), [DPT](../../en/model_doc/dpt), [MobileNetV2](../../en/model_doc/mobilenet_v2), [MobileViT](../../en/model_doc/mobilevit), [MobileViTV2](../../en/model_doc/mobilevitv2), [SegFormer](../../en/model_doc/segformer), [UPerNet](../../en/model_doc/upernet)

<!--End of the generated tip-->

</Tip>

시작하기 전에 필요한 라이브러리가 모두 설치되어 있는지 확인하세요:

```bash
pip install -q datasets transformers evaluate
```

모델을 커뮤니티와 공유하려면 Hugging Face 계정에 로그인하는 것을 권장합니다. 로그인을 요청받으면 토큰을 입력하여 로그인하세요:

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## SceneParse150 데이터셋 불러오기 [[load-sceneparse150-dataset]]

먼저, 🤗 Datasets 라이브러리에서 작은 SceneParse150 데이터셋 하위 집합을 로드합니다. 이렇게 하면 전체 데이터셋을 사용하기 전에 실험해보고 모든 것이 올바르게 작동하는지 확인할 수 있습니다.

```py
>>> from datasets import load_dataset

>>> ds = load_dataset("scene_parse_150", split="train[:50]")
```

데이터셋의 `train` 분할을 [`~datasets.Dataset.train_test_split`] 메서드를 사용하여 훈련 및 테스트 세트로 분할합니다.

```py
>>> ds = ds.train_test_split(test_size=0.2)
>>> train_ds = ds["train"]
>>> test_ds = ds["test"]
```

그런 다음, 예시를 살펴보세요:

```py
>>> train_ds[0]
{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x683 at 0x7F9B0C201F90>,
 'annotation': <PIL.PngImagePlugin.PngImageFile image mode=L size=512x683 at 0x7F9B0C201DD0>,
 'scene_category': 368}
```

- `image`: 장면의 PIL 이미지.
- `annotation`: 세그멘테이션 맵의 PIL 이미지, 모델의 목표로 사용됩니다.
- `scene_category`: "부엌"이나 "사무실"과 같은 이미지 장면을 나타내는 카테고리 ID입니다. 이 가이드에서는 `image`와 `annotation`만 필요합니다. 두 이미지 모두 PIL 이미지입니다.

이후 모델 설정 시 레이블 ID를 레이블 클래스로 매핑하는 사전을 만들 필요가 있습니다. 매핑은 허브에서 다운로드하여 `id2label` 및 `label2id` 사전을 생성합니다.

```py
>>> import json
>>> from huggingface_hub import cached_download, hf_hub_url

>>> repo_id = "huggingface/label-files"
>>> filename = "ade20k-id2label.json"
>>> id2label = json.load(open(cached_download(hf_hub_url(repo_id, filename, repo_type="dataset")), "r"))
>>> id2label = {int(k): v for k, v in id2label.items()}
>>> label2id = {v: k for k, v in id2label.items()}
>>> num_labels = len(id2label)
```

## 데이터 전처리 [[preprocess]]

다음 단계는 SegFormer 이미지 프로세서를 사용하여 이미지와 어노테이션을 모델에 입력 가능한 형식으로 변환하는 것입니다. 일부 데이터셋은 배경 클래스로 제로 인덱스를 사용하는 경우가 있습니다. 그러나 150개의 클래스 중 배경 클래스가 없으므로 모든 레이블에서 1을 빼야 합니다. 이를 위해 `reduce_labels=True`로 설정하여 제로 인덱스를 `255`로 대체하여 SegFormer 손실 함수에서 배경 클래스를 무시하도록 합니다.

```py
>>> from transformers import AutoImageProcessor

>>> checkpoint = "nvidia/mit-b0"
>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint, reduce_labels=True)
```

<frameworkcontent>
<pt>

모델이 과적합을 피하기 위해 이미지 데이터셋에 데이터 증강을 적용하는 것은 흔한 일입니다. 이 가이드에서는 [torchvision](https://pytorch.org/vision/stable/index.html) 라이브러리의 [`ColorJitter`](https://pytorch.org/vision/stable/generated/torchvision.transforms.ColorJitter.html) 함수를 사용하여 이미지의 색상 속성을 무작위로 변경할 것입니다. 하지만 다른 이미지 라이브러리를 사용해도 무방합니다.

```py
>>> from torchvision.transforms import ColorJitter

>>> jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)
```

먼저, 이미지를 `pixel_values`로 변환하고 어노테이션을 `labels`로 변환하는 두 개의 전처리 함수를 생성합니다. 훈련 세트에 대해 이미지 프로세서에 `jitter`를 적용한 다음 이미지를 제공합니다. 테스트 세트에 대해서는 이미지 프로세서가 이미지를 잘라내고 정규화하며 테스트 중에는 데이터 증강을 적용하지 않습니다.

```py
>>> def train_transforms(example_batch):
...     images = [jitter(x) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs


>>> def val_transforms(example_batch):
...     images = [x for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs
```

데이터셋 전체에 전처리 변환을 적용하려면 🤗 Datasets [`~datasets.Dataset.set_transform`] 함수를 사용하세요.
이 변환은 실시간으로 적용되며 디스크 공간을 더 적게 사용합니다:

```py
>>> train_ds.set_transform(train_transforms)
>>> test_ds.set_transform(val_transforms)
```

</pt>
</frameworkcontent>

<frameworkcontent>
<tf>
데이터 증강을 적용하여 모델이 과적합을 더 잘 피하도록 이미지 데이터셋을 변환하는 것은 일반적입니다.
이 안내서에서는 [`tf.image`](https://www.tensorflow.org/api_docs/python/tf/image)를 사용하여 이미지의 색상 특성을 무작위로 변경할 것입니다. 하지만 다른 이미지 라이브러리를 사용해도 괜찮습니다.
다음은 두 가지 다른 변환 함수를 정의하세요:
- 이미지 증강을 포함하는 훈련 데이터 변환
- 컴퓨터 비전 모델에서는 채널 순서를 먼저 사용하는 것이 일반적이므로 검증 데이터 변환은 이미지를 전치(transpose)하는 것만 포함합니다.

```py
>>> import tensorflow as tf


>>> def aug_transforms(image):
...     image = tf.keras.utils.img_to_array(image)
...     image = tf.image.random_brightness(image, 0.25)
...     image = tf.image.random_contrast(image, 0.5, 2.0)
...     image = tf.image.random_saturation(image, 0.75, 1.25)
...     image = tf.image.random_hue(image, 0.1)
...     image = tf.transpose(image, (2, 0, 1))
...     return image


>>> def transforms(image):
...     image = tf.keras.utils.img_to_array(image)
...     image = tf.transpose(image, (2, 0, 1))
...     return image
```

이제 이미지와 어노테이션을 모델에 입력할 수 있도록 두 가지 전처리 함수를 만들어야 합니다. 이 함수는 이미지를 `pixel_values`로 변환하고 어노테이션을 `labels`로 변환합니다. 훈련 세트의 경우 이미지 프로세서가 이미지에 `jitter`를 적용한 후 이미지를 입력으로 사용합니다. 테스트 세트의 경우 이미지 프로세서가 이미지를 잘라내고 정규화하며 테스트 중에는 데이터 증강을 적용하지 않습니다.

```py
>>> def train_transforms(example_batch):
...     images = [aug_transforms(x.convert("RGB")) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs


>>> def val_transforms(example_batch):
...     images = [transforms(x.convert("RGB")) for x in example_batch["image"]]
...     labels = [x for x in example_batch["annotation"]]
...     inputs = image_processor(images, labels)
...     return inputs
```

전체 데이터셋에 전처리 변환을 적용하려면 🤗 Datasets [`~datasets.Dataset.set_transform`] 함수를 사용하세요.
변환은 실시간으로 적용되며 디스크 공간을 적게 사용합니다:

```py
>>> train_ds.set_transform(train_transforms)
>>> test_ds.set_transform(val_transforms)
```
</tf>
</frameworkcontent>

## 평가 [[evaluate]]

훈련 중에 메트릭을 포함하는 것은 모델의 성능을 평가하는 데 도움이 됩니다. 빠르게 평가 메서드를 로드하기 위해 🤗 [Evaluate](https://huggingface.co/docs/evaluate/index) 라이브러리를 사용할 수 있습니다. 이 작업을 위해 [평균 IoU(Intersection over Union)](https://huggingface.co/spaces/evaluate-metric/accuracy) 메트릭을 로드하세요 (더 많은 정보를 알아보려면 🤗 Evaluate [빠른 투어](https://huggingface.co/docs/evaluate/a_quick_tour)를 참조하세요):

```py
>>> import evaluate

>>> metric = evaluate.load("mean_iou")
```

그런 다음 [`~evaluate.EvaluationModule.compute`] 메서드를 사용하여 메트릭을 계산하는 함수를 생성하세요. 예측값을 먼저 로짓으로 변환하고, [`~evaluate.EvaluationModule.compute`]을 호출하기 전에 레이블 크기와 일치하도록 재구성해야 합니다:

<frameworkcontent>
<pt>

```py
>>> import numpy as np
>>> import torch
>>> from torch import nn

>>> def compute_metrics(eval_pred):
...     with torch.no_grad():
...         logits, labels = eval_pred
...         logits_tensor = torch.from_numpy(logits)
...         logits_tensor = nn.functional.interpolate(
...             logits_tensor,
...             size=labels.shape[-2:],
...             mode="bilinear",
...             align_corners=False,
...         ).argmax(dim=1)

...         pred_labels = logits_tensor.detach().cpu().numpy()
...         metrics = metric.compute(
...             predictions=pred_labels,
...             references=labels,
...             num_labels=num_labels,
...             ignore_index=255,
...             reduce_labels=False,
...         )
...         for key, value in metrics.items():
...             if type(value) is np.ndarray:
...                 metrics[key] = value.tolist()
...         return metrics
```

</pt>
</frameworkcontent>


<frameworkcontent>
<tf>

```py
>>> def compute_metrics(eval_pred):
...     logits, labels = eval_pred
...     logits = tf.transpose(logits, perm=[0, 2, 3, 1])
...     logits_resized = tf.image.resize(
...         logits,
...         size=tf.shape(labels)[1:],
...         method="bilinear",
...     )

...     pred_labels = tf.argmax(logits_resized, axis=-1)
...     metrics = metric.compute(
...         predictions=pred_labels,
...         references=labels,
...         num_labels=num_labels,
...         ignore_index=-1,
...         reduce_labels=image_processor.do_reduce_labels,
...     )

...     per_category_accuracy = metrics.pop("per_category_accuracy").tolist()
...     per_category_iou = metrics.pop("per_category_iou").tolist()

...     metrics.update({f"accuracy_{id2label[i]}": v for i, v in enumerate(per_category_accuracy)})
...     metrics.update({f"iou_{id2label[i]}": v for i, v in enumerate(per_category_iou)})
...     return {"val_" + k: v for k, v in metrics.items()}
```

</tf>
</frameworkcontent>

`compute_metrics` 함수가 준비되었으므로 모델 설정 시 이 함수를 다시 사용하게 될 것입니다.

## 훈련 [[train]]
<frameworkcontent>
<pt>
<Tip>

[`Trainer`]를 사용하여 모델을 파인튜닝하는 방법에 익숙하지 않다면, [여기](../training#finetune-with-trainer)의 기본 튜토리얼을 확인하세요!

</Tip>

이제 모델을 훈련할 준비가 되었습니다! [`AutoModelForSemanticSegmentation`]를 사용하여 SegFormer 모델을 로드하고 레이블 ID와 레이블 클래스 간의 매핑을 모델에 전달하세요:

```py
>>> from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer

>>> model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id=label2id)
```

이 시점에서 세 가지 단계만 남았습니다:

1. [`TrainingArguments`]에서 훈련 하이퍼파라미터를 정의하세요. 사용하지 않는 열을 제거하지 않도록 주의하세요. 이렇게 하면 `image` 열이 삭제됩니다. `image` 열 없이 `pixel_values`를 생성할 수 없습니다. `remove_unused_columns=False`로 설정하여 이 동작을 방지하세요! 또한 유일한 필수 매개변수는 모델을 저장할 위치를 지정하는 `output_dir`입니다. 이 모델을 허브로 업로드하려면 `push_to_hub=True`로 설정하세요 (모델을 업로드하려면 Hugging Face에 로그인해야 합니다). 각 에포크의 끝에서 [`Trainer`]는 IoU 메트릭을 평가하고 훈련 체크포인트를 저장합니다.
2. 훈련 인자를 [`Trainer`]에 모델, 데이터셋, 토크나이저, 데이터 콜레이터 및 `compute_metrics` 함수와 함께 전달하세요.
3. [`~Trainer.train`]을 호출하여 모델을 파인튜닝하세요.

```py
>>> training_args = TrainingArguments(
...     output_dir="segformer-b0-scene-parse-150",
...     learning_rate=6e-5,
...     num_train_epochs=50,
...     per_device_train_batch_size=2,
...     per_device_eval_batch_size=2,
...     save_total_limit=3,
...     evaluation_strategy="steps",
...     save_strategy="steps",
...     save_steps=20,
...     eval_steps=20,
...     logging_steps=1,
...     eval_accumulation_steps=5,
...     remove_unused_columns=False,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=train_ds,
...     eval_dataset=test_ds,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

훈련이 완료되면 [`~transformers.Trainer.push_to_hub`] 메서드를 사용하여 모델을 허브에 공유하세요. 이렇게 하면 누구나 모델을 사용할 수 있습니다:

```py
>>> trainer.push_to_hub()
```
</pt>
</frameworkcontent>

<frameworkcontent>
<tf>
<Tip>

케라스를 사용하여 모델을 파인튜닝하는 방법에 익숙하지 않다면, [기본 튜토리얼](./training#train-a-tensorflow-model-with-keras)을 먼저 확인하세요!

</Tip>

TensorFlow에서 모델을 파인튜닝하려면 다음 단계를 따르세요:
1. 훈련 하이퍼파라미터를 정의하고 옵티마이저와 학습률 스케줄을 설정합니다.
2. 사전 훈련된 모델을 인스턴스화합니다.
3. 🤗 Dataset을 `tf.data.Dataset` 형식으로 변환합니다.
4. 모델을 컴파일합니다.
5. 메트릭을 계산하고 모델을 🤗 Hub에 업로드하기 위한 콜백을 추가합니다.
6. `fit()` 메서드를 사용하여 훈련을 실행합니다.

먼저, 하이퍼파라미터, 옵티마이저 및 학습률 스케줄을 정의하세요:

```py
>>> from transformers import create_optimizer

>>> batch_size = 2
>>> num_epochs = 50
>>> num_train_steps = len(train_ds) * num_epochs
>>> learning_rate = 6e-5
>>> weight_decay_rate = 0.01

>>> optimizer, lr_schedule = create_optimizer(
...     init_lr=learning_rate,
...     num_train_steps=num_train_steps,
...     weight_decay_rate=weight_decay_rate,
...     num_warmup_steps=0,
... )
```

그런 다음, 레이블 매핑과 함께 [`TFAutoModelForSemanticSegmentation`]를 사용하여 SegFormer를 로드하고 옵티마이저와 함께 컴파일하세요.
Transformer 모델은 모두 기본 작업 관련 손실 함수를 가지고 있으므로 필요한 경우 지정하지 않아도 됩니다:

```py
>>> from transformers import TFAutoModelForSemanticSegmentation

>>> model = TFAutoModelForSemanticSegmentation.from_pretrained(
...     checkpoint,
...     id2label=id2label,
...     label2id=label2id,
... )
>>> model.compile(optimizer=optimizer)  # No loss argument!
```

데이터셋을 [`~datasets.Dataset.to_tf_dataset`] 및 [`DefaultDataCollator`]를 사용하여 `tf.data.Dataset` 형식으로 변환하세요:

```py
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator(return_tensors="tf")

>>> tf_train_dataset = train_ds.to_tf_dataset(
...     columns=["pixel_values", "label"],
...     shuffle=True,
...     batch_size=batch_size,
...     collate_fn=data_collator,
... )

>>> tf_eval_dataset = test_ds.to_tf_dataset(
...     columns=["pixel_values", "label"],
...     shuffle=True,
...     batch_size=batch_size,
...     collate_fn=data_collator,
... )
```

예측에서 정확도를 계산하고 모델을 🤗 Hub에 업로드하려면 [케라스 콜백](../main_classes/keras_callbacks)을 사용하세요.
`compute_metrics` 함수를 [`KerasMetricCallback`]에 전달하고 [`PushToHubCallback`]을 사용하여 모델을 업로드하세요:

```py
>>> from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback

>>> metric_callback = KerasMetricCallback(
...     metric_fn=compute_metrics, eval_dataset=tf_eval_dataset, batch_size=batch_size, label_cols=["labels"]
... )

>>> push_to_hub_callback = PushToHubCallback(output_dir="scene_segmentation", tokenizer=image_processor)

>>> callbacks = [metric_callback, push_to_hub_callback]
```

마지막으로, 모델을 훈련할 준비가 되었습니다! `fit()`을 호출하여 훈련 및 검증 데이터셋, 에포크 수 및 콜백을 사용하여 모델을 파인튜닝하세요:

```py
>>> model.fit(
...     tf_train_dataset,
...     validation_data=tf_eval_dataset,
...     callbacks=callbacks,
...     epochs=num_epochs,
... )
```

축하합니다! 모델을 파인튜닝하고 🤗 Hub에 공유하였습니다. 이제 추론에 사용할 수 있습니다!
</tf>
</frameworkcontent>


## 추론 [[inference]]

좋습니다, 이제 모델을 파인튜닝했으므로 추론에 활용할 수 있습니다!

추론용 이미지를 불러오세요:

```py
>>> image = ds[0]["image"]
>>> image
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-image.png" alt="침실 이미지"/>
</div>

<frameworkcontent>
<pt>
파인튜닝한 모델을 추론에 간단히 활용하는 가장 간단한 방법은 [`pipeline`]을 사용하는 것입니다. 모델을 사용하여 이미지 세그멘테이션을 위한 `pipeline`을 인스턴스화하고 이미지를 전달하세요:

```py
>>> from transformers import pipeline

>>> segmenter = pipeline("image-segmentation", model="my_awesome_seg_model")
>>> segmenter(image)
[{'score': None,
  'label': 'wall',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062690>},
 {'score': None,
  'label': 'sky',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062A50>},
 {'score': None,
  'label': 'floor',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062B50>},
 {'score': None,
  'label': 'ceiling',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062A10>},
 {'score': None,
  'label': 'bed ',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062E90>},
 {'score': None,
  'label': 'windowpane',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062390>},
 {'score': None,
  'label': 'cabinet',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062550>},
 {'score': None,
  'label': 'chair',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062D90>},
 {'score': None,
  'label': 'armchair',
  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062E10>}]
```

원하는 경우 `pipeline`의 결과를 직접 재현할 수도 있습니다. 이미지 프로세서를 사용하여 이미지를 처리하고 `pixel_values`를 GPU에 넣은 다음 모델에 입력하세요:

```py
>>> device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # use GPU if available, otherwise use a CPU
>>> encoding = image_processor(image, return_tensors="pt")
>>> pixel_values = encoding.pixel_values.to(device)
```

입력을 모델에 전달하고 `logits`을 얻으세요:

```py
>>> outputs = model(pixel_values=pixel_values)
>>> logits = outputs.logits.cpu()
```

그런 다음 `logits`을 원래 이미지 크기로 다시 스케일링하세요:

```py
>>> upsampled_logits = nn.functional.interpolate(
...     logits,
...     size=image.size[::-1],
...     mode="bilinear",
...     align_corners=False,
... )

>>> pred_seg = upsampled_logits.argmax(dim=1)[0]
```

</pt>
</frameworkcontent>

<frameworkcontent>
<tf>
모델을 사용하여 추론해 보려면 이미지 전처리기를 사용하여 이미지를 처리하고 TensorFlow 텐서로 입력을 반환하세요:

```py
>>> from transformers import AutoImageProcessor

>>> image_processor = AutoImageProcessor.from_pretrained("MariaK/scene_segmentation")
>>> inputs = image_processor(image, return_tensors="tf")
```

모델에 입력을 전달하고 `logits`을 얻으세요:

```py
>>> from transformers import TFAutoModelForSemanticSegmentation

>>> model = TFAutoModelForSemanticSegmentation.from_pretrained("MariaK/scene_segmentation")
>>> logits = model(**inputs).logits
```

그런 다음 `logits`을 원래 이미지 크기로 다시 스케일링하고 클래스 차원에서 argmax를 적용하세요:
```py
>>> logits = tf.transpose(logits, [0, 2, 3, 1])

>>> upsampled_logits = tf.image.resize(
...     logits,
...     # We reverse the shape of `image` because `image.size` returns width and height.
...     image.size[::-1],
... )

>>> pred_seg = tf.math.argmax(upsampled_logits, axis=-1)[0]
```

</tf>
</frameworkcontent>

결과를 시각화하려면 [데이터셋 컬러 팔레트](https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/deeplab/utils/get_dataset_colormap.py#L51)를 불러와 각 클래스를 RGB 값에 매핑하는 `ade_palette()` 함수를 사용하세요. 그런 다음 이미지와 예측된 세그멘테이션 맵을 결합하여 플롯하세요:

```py
>>> import matplotlib.pyplot as plt
>>> import numpy as np

>>> color_seg = np.zeros((pred_seg.shape[0], pred_seg.shape[1], 3), dtype=np.uint8)
>>> palette = np.array(ade_palette())
>>> for label, color in enumerate(palette):
...     color_seg[pred_seg == label, :] = color
>>> color_seg = color_seg[..., ::-1]  # convert to BGR

>>> img = np.array(image) * 0.5 + color_seg * 0.5  # plot the image with the segmentation map
>>> img = img.astype(np.uint8)

>>> plt.figure(figsize=(15, 10))
>>> plt.imshow(img)
>>> plt.show()
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-preds.png" alt="침실 이미지 위에 세그멘테이션 맵 겹쳐 보기"/>
</div>
